{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GANs: Generative Adversarial Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative models: \n",
    "\n",
    "The most popular types are:\n",
    "- **VAE**: Variational autoencoders [https://arxiv.org/abs/1804.00891]:\n",
    "    - Work with 2 models, which are typically NNs.\n",
    "        - **Encoder**: Realistic images are fed, and its job is to represent it in a hyperspherical latent space.\n",
    "        - **Decoder**: Reconstructs the image that the encoder saw from the vector in the latent space.\n",
    "    - After training, we actually lop off the encoder and we can pick random points in the latent space.\n",
    "    - The variational part actually inject some noise into this whole model and training process. Instead of having the encoder encode the image into a single point in that latent space, the encoder actually encodes the image onto a whole distribution and then samples a point on that distribution to feed into the decoder to then produce a realistic image. This adds a little bit of noise since different points can be sampled on this distribution.\n",
    "    \n",
    "    \n",
    "- **GANs**: There are 2 models behind a GAN:\n",
    "    - **Generator** (*art forger*): similar to encoder: there's no guiding encoder this time that determines what noise vector should look like, that's input into the generator. Instead, there's a discriminator.\n",
    "        - Given noise and class Y, it's goal is to generate a set of features X that look realistic.\n",
    "        - Learns to make fakes.\n",
    "        - Isn't allowed to see real images.\n",
    "        - Noise (random values) its used to make sure that what's generated is not the same each time.\n",
    "        - Try to capture *P(X|Y)*.\n",
    "\n",
    "    - **Discriminator** (*art inspector*): looks at fake and real images and simultaneously trying to figure out which ones are real and which ones are fake.\n",
    "        - Learns to distinguish real from fake.\n",
    "        - It can see real images, but doesn't know which is real and which isn't.\n",
    "        - It is a **classifier**, distinguishing between classes (real and fake). *P(Y|X)*.       \n",
    "    \n",
    "    - They \"fight\" against each other, making 1 of the models so good that it generates realistic images. They learn from each other until they reach a point where we don't need the discriminator anymore, and the generator can take in any random noise and produce a realistic image.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intuition Behind GANs:\n",
    "\n",
    "1. We train the discriminator using real artwork. After it decides (real or fake), we tell it the truth.\n",
    "2. When the generator starts creating, it will know in what direction to go on and improve by looking at the scores assigned to its work by the discriminator. \n",
    "3. And the discriminator also improves over time because it receives more and more realistic images at each round from the generator, but it will reach a point when generated images are too good to distinguish.\n",
    "4. When we are happy with the result of this generator, the process ends. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But how does the generator improve over time?\n",
    "\n",
    "<img src=\"images/gan_seq.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generated image $\\hat{X}$ is fed into the discriminator, which outputs its predictions $\\hat{Y}$<sub>d</sub> (how real or fake it thinks generated image is). We can compute a **Cost function** out of this, which looks at how far the examples produced by the generator are being considered real by the discriminator:\n",
    "\n",
    "- The generator wants $\\hat{Y} = 1$, as real as possible.\n",
    "- The discriminator is trying to get $\\hat{Y} = 0$, fake. \n",
    "\n",
    "The difference between these two is used to update the parameters of the generator, making it to improve over\n",
    "time and know which direction to move it's parameters to generate something that looks more real (and that will fool the discriminator). Once we achieve this we can save the parameters theta of the generator, freezing them so that we can load them back up and sample new images.\n",
    "\n",
    "The generator will try to approximate the real distribution seen in training. For example, if we are generating cats, most common breeds will have more chances of being generated. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\hat{Y}$<sub>d</sub>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
